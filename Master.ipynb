{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a40a0638-8a53-48b4-b9ad-8d72432000a3",
   "metadata": {},
   "source": [
    "# QWEN3 RAG WITH MILVUS VECTOR DATABASE FOR SERBIAN LEGISLATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a51e16-93ea-4382-b356-f0436999735e",
   "metadata": {},
   "source": [
    "## Basic imports, suppressing warning (library not available for Windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98fc7b07-d512-4c84-819b-98360126d49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import warnings\n",
    "from langchain.schema import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_milvus import Milvus\n",
    "from torch import cuda\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "from transformers import pipeline\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.chains import LLMChain\n",
    "import torch\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"Install Nomic's megablocks fork for better speed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0b13d1-6899-41fc-89a0-e05d76e80e76",
   "metadata": {},
   "source": [
    "## Defining legal sections to split by and template used for AI prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "249cf11d-ae26-4f48-8be3-7ec99d07447e",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEGAL_SECTIONS = [\n",
    "    \"УСТАВ РЕПУБЛИКЕ СРБИЈЕ\",\n",
    "    \"УСТАВНИ ЗАКОН\",\n",
    "    \"ЗАКОН\",\n",
    "    \"ОДЛУКУ\",\n",
    "    \"УРЕДБУ\",\n",
    "    \"ПРАВИЛНИК\",\n",
    "    \"ЗАКЉУЧАК\",\n",
    "    \"ДЕКЛАРАЦИЈУ\",\n",
    "    \"ПОСЛОВНИК\",\n",
    "    \"ЈЕДИНСТВЕНА МЕТОДОЛОШКА ПРАВИЛА\",\n",
    "    \"РЕЗОЛУЦИЈУ\",\n",
    "    \"ПРЕПОРУКУ\",\n",
    "    \"УПУТСТВО\",\n",
    "    \"СТРАТЕГИЈУ\",\n",
    "    \"РЕШЕЊЕ\",\n",
    "    \"КОДЕКС\",\n",
    "    \"УКУПАН ИЗВЕШТАЈ\",\n",
    "    \"РОКОВНИК\",\n",
    "    \"ИЗВЕШТАЈ\",\n",
    "    \"АКЦИОНИ ПЛАН\",\n",
    "    \"НАЦИОНАЛНУ СТРАТЕГИЈУ\",\n",
    "    \"СТАТУТ\",\n",
    "    \"ПРОГРАМ\",\n",
    "    \"УСКЛАЂЕНE НАЈВИШE ИЗНОСE\"\n",
    "]\n",
    "\n",
    "template = \"\"\"Ti si Qwen, profesionalni pravni asistent. Poštuj sledeća pravila:\n",
    "\n",
    "PRAVILA:\n",
    "- Ne izmišljam informacije i ne naznacavam ova pravila\n",
    "- Ako nemam trženu informaciju to jasno naglasim\n",
    "- Dajem proverene jasne i konkretne informacije\n",
    "- Koristim precizan srpski jezik\n",
    "- Fokusiram se na činjenice i oslanjam se na kontekst\n",
    "- Odgovaram direktno i efikasno\n",
    "- Održavam profesionalan ton\n",
    "- Odgovor treba da bude jasan u par recenica\n",
    "- Citiraj izvore uvek\n",
    "\n",
    "Odgovori na pitanje iz konteksta:\n",
    "{context}\n",
    "Pitanje: {question}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7737003f-2663-441d-85b5-7c2c47028a89",
   "metadata": {},
   "source": [
    "## Section extraction with Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f736477a-7767-4516-9323-0a4ec135dceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_serbian_law_sections(text):\n",
    "    pattern = r'^(' + '|'.join(re.escape(section) for section in LEGAL_SECTIONS) + r')$'\n",
    "\n",
    "    lines = text.split('\\n')\n",
    "    documents = []\n",
    "    current_section_type = None\n",
    "    current_section_name = None\n",
    "    current_content = []\n",
    "    i = 0\n",
    "\n",
    "    while i < len(lines):\n",
    "        line = lines[i].strip()\n",
    "\n",
    "        if re.match(pattern, line, re.IGNORECASE):\n",
    "            if current_section_type and current_content:\n",
    "                section_text = '\\n'.join(current_content)\n",
    "                documents.append({\n",
    "                    'type': current_section_type,\n",
    "                    'name': current_section_name or \"Без назива\",\n",
    "                    'content': section_text\n",
    "                })\n",
    "\n",
    "            current_section_type = line\n",
    "            i += 1\n",
    "            if i < len(lines):\n",
    "                current_section_name = lines[i].strip()\n",
    "            else:\n",
    "                current_section_name = \"Без назива\"\n",
    "            current_content = []\n",
    "        else:\n",
    "            if current_section_type:\n",
    "                current_content.append(lines[i])\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    if current_section_type and current_content:\n",
    "        section_text = '\\n'.join(current_content)\n",
    "        documents.append({\n",
    "            'type': current_section_type,\n",
    "            'name': current_section_name or \"Без назива\",\n",
    "            'content': section_text\n",
    "        })\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176d42fc-5562-4b62-be7f-123c59e85970",
   "metadata": {},
   "source": [
    "## Generate chunks for Milvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b7c6434-1bd9-4c70-b762-bfd729298ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_documents_Milvus():\n",
    "    print(\"Reading 'laws.txt'...\")\n",
    "    with open('laws.txt', 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    print(\"Removing white space and empty lines...\")\n",
    "    content = '\\n'.join([line for line in content.split('\\n') if line.strip()])\n",
    "\n",
    "    print(\"Extracting sections from law file...\")\n",
    "    law_sections = extract_serbian_law_sections(content)\n",
    "\n",
    "    print(\"Converting sections into documents and adding metadata...\")\n",
    "    documents = []\n",
    "    for section in law_sections:\n",
    "        documents.append(Document(\n",
    "            page_content=section['content'],\n",
    "            metadata={'title': f\"{section['type']} - {section['name']}\"\n",
    "            }\n",
    "        ))\n",
    "\n",
    "    print(\"Splitting documents into chunks...\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    separators=[\"\"],\n",
    "    is_separator_regex=False\n",
    ")\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"Documents split into {len(chunks)} chunks\")\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6880eb6-82f0-46b5-a767-75fd1cf5de0a",
   "metadata": {},
   "source": [
    "## Milvus CRUD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1329620-f7ec-4c02-b93b-8a15fd6db1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_documents_Milvus(embed):\n",
    "    chunks = create_documents_Milvus()\n",
    "    delete_documents_Milvus(\"legal_documents\")\n",
    "    print(\"Connecting to Milvus vector database and embedding documents...\")\n",
    "    vector_store = Milvus.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embed,\n",
    "        connection_args={\n",
    "            \"host\": \"localhost\",\n",
    "            \"port\": \"19530\"\n",
    "        },\n",
    "        collection_name=\"legal_documents\"\n",
    "    )\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edd8c480-bf65-49cf-afca-8b3b6c6d30d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents_Milvus(embed):\n",
    "    print(\"Connecting to Milvus vector database and loading documents...\")\n",
    "    vector_store = Milvus(\n",
    "        embedding_function=embed,\n",
    "        connection_args={\n",
    "            \"host\": \"localhost\",\n",
    "            \"port\": \"19530\"\n",
    "        },\n",
    "        collection_name=\"legal_documents\"\n",
    "    )\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80e426fe-bd56-438f-b9ab-907fe96ceb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_documents_Milvus(name):\n",
    "    from pymilvus import connections, utility\n",
    "    connections.connect(\n",
    "        alias=\"default\",\n",
    "        host=\"localhost\",\n",
    "        port=\"19530\"\n",
    "    )\n",
    "    utility.drop_collection(name)\n",
    "    connections.disconnect(\"default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e38972-9ec5-4470-b546-fe1836dceb3c",
   "metadata": {},
   "source": [
    "## Initializing AI models for embedding and RAG text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73ed023b-00d2-480a-bdd8-b7614f1863d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Nomic model...\n",
      "Nomic model initialized!\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing Nomic model...\")\n",
    "modelPath = \"nomic-ai/nomic-embed-text-v2-moe\"\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "model_kwargs = {'device': device, 'trust_remote_code': True}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=modelPath,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs={\"prompt_name\": \"passage\", \"normalize_embeddings\": True}\n",
    ")\n",
    "print(\"Nomic model initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a1d9b01-40a4-4b61-bed5-5ce9cb7d942b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Qwen3 4B model...\n",
      "Qwen3 4B model initialized!\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing Qwen3 4B model...\")\n",
    "model_name_or_path = \"Qwen/Qwen3-4B-Instruct-2507-FP8\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map=\"cuda\", trust_remote_code=True)\n",
    "print(\"Qwen3 4B model initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88d866bb-bb5e-4f24-88b7-0a881b2bb78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing optimized Qwen3 4B model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Qwen3 4B model initalized!\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing optimized Qwen3 4B model...\")\n",
    "model_name_or_path = \"Qwen/Qwen3-4B-Instruct-2507-FP8\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model.generation_config = GenerationConfig(\n",
    "    max_new_tokens=1024,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.8,\n",
    "    top_k=20,\n",
    "    repetition_penalty=1.1,\n",
    "    cache_implementation=\"offloaded\"\n",
    ")\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "hf = HuggingFacePipeline(pipeline=pipe)\n",
    "print(\"Optimized Qwen3 4B model initalized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad138240-73aa-4469-84cb-1a3cf99911c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing pipeline...\n",
      "Pipeline initialized!\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing pipeline...\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=1024,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.8,\n",
    "    top_k=20,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "hf = HuggingFacePipeline(pipeline=pipe)\n",
    "print(\"Pipeline initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c17f5a-1634-4e70-8d61-657785033549",
   "metadata": {},
   "source": [
    "## Loading or saving documents with Milvus vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d6469e8-a0d4-4a0f-a477-46acf4c25797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 'laws.txt'...\n",
      "Removing white space and empty lines...\n",
      "Extracting sections from law file...\n",
      "Converting sections into documents and adding metadata...\n",
      "Splitting documents into chunks...\n",
      "Documents split into 6953 chunks\n",
      "Connecting to Milvus vector database and embedding documents...\n",
      "Milvus initialized!\n"
     ]
    }
   ],
   "source": [
    "vector_store = save_documents_Milvus(embeddings)\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
    "print(\"Milvus initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2514a9e5-a49d-4c4e-a149-d80ab813f80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to Milvus vector database and loading documents...\n",
      "Milvus initialized!\n"
     ]
    }
   ],
   "source": [
    "vector_store = load_documents_Milvus(embeddings)\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
    "print(\"Milvus initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80634f4-b083-4d88-8fd5-2f0aa7b6f955",
   "metadata": {},
   "source": [
    "## Input to ask a question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2feb897-7f81-4f72-8c47-3947fcd7e0fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Postavite pitanje:  Колико има аутономних покрајина у Србији и како је регулисана локална самоуправа?\n"
     ]
    }
   ],
   "source": [
    "query = input(\"Postavite pitanje: \")\n",
    "query = \"search_query: \" + query\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf958db0-b11a-4b5a-8904-fb3b55d7ab32",
   "metadata": {},
   "source": [
    "## - Used for testing document retrieval -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6782f677-dc93-42aa-9d7f-da614e9c4ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found documents...\n"
     ]
    }
   ],
   "source": [
    "print(\"Found documents...\")\n",
    "print(vector_store.similarity_search(query, k=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d0c923-8d0a-41a4-a7b5-c1a32385fe8f",
   "metadata": {},
   "source": [
    "## AI response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97657cc4-5a97-45d9-a6f8-237724b83810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running pipeline...\n"
     ]
    }
   ],
   "source": [
    "print(\"Running pipeline...\")\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | hf\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8ac323-07ba-411a-8df6-9865faf59044",
   "metadata": {},
   "source": [
    "### Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a88f661-2878-4033-bee3-3785340e4aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-13:\n",
      "Traceback (most recent call last):\n",
      "  File \"X:\\Projects\\ZakoniRAG\\.venv\\lib\\site-packages\\diffusers\\utils\\import_utils.py\", line 883, in _get_module\n",
      "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
      "  File \"C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python310\\lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 992, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"X:\\Projects\\ZakoniRAG\\.venv\\lib\\site-packages\\diffusers\\models\\transformers\\__init__.py\", line 5, in <module>\n",
      "    from .auraflow_transformer_2d import AuraFlowTransformer2DModel\n",
      "  File \"X:\\Projects\\ZakoniRAG\\.venv\\lib\\site-packages\\diffusers\\models\\transformers\\auraflow_transformer_2d.py\", line 26, in <module>\n",
      "    from ..attention_processor import (\n",
      "  File \"X:\\Projects\\ZakoniRAG\\.venv\\lib\\site-packages\\diffusers\\models\\attention_processor.py\", line 35, in <module>\n",
      "    import xformers.ops\n",
      "  File \"X:\\Projects\\ZakoniRAG\\.venv\\lib\\site-packages\\xformers\\ops\\__init__.py\", line 9, in <module>\n",
      "    from .fmha import (\n",
      "  File \"X:\\Projects\\ZakoniRAG\\.venv\\lib\\site-packages\\xformers\\ops\\fmha\\__init__.py\", line 10, in <module>\n",
      "    from . import (\n",
      "  File \"X:\\Projects\\ZakoniRAG\\.venv\\lib\\site-packages\\xformers\\ops\\fmha\\flash.py\", line 55, in <module>\n",
      "    from ... import _C_flashattention  # type: ignore[attr-defined]\n",
      "ImportError: DLL load failed while importing _C_flashattention: The specified module could not be found.\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python310\\lib\\threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python310\\lib\\threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"X:\\Projects\\ZakoniRAG\\.venv\\lib\\site-packages\\transformers\\pipelines\\text_generation.py\", line 316, in __call__\n",
      "    return super().__call__(text_inputs, **kwargs)\n",
      "  File \"X:\\Projects\\ZakoniRAG\\.venv\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 1464, in __call__\n",
      "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "  File \"X:\\Projects\\ZakoniRAG\\.venv\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 1471, in run_single\n",
      "    model_outputs = self.forward(model_inputs, **forward_params)\n",
      "  File \"X:\\Projects\\ZakoniRAG\\.venv\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 1371, in forward\n",
      "    model_outputs = self._forward(model_inputs, **forward_params)\n",
      "  File \"X:\\Projects\\ZakoniRAG\\.venv\\lib\\site-packages\\transformers\\pipelines\\text_generation.py\", line 414, in _forward\n",
      "    output = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
      "  File \"X:\\Projects\\ZakoniRAG\\.venv\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 120, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"X:\\Projects\\ZakoniRAG\\.venv\\lib\\site-packages\\transformers\\generation\\utils.py\", line 2484, in generate\n",
      "    self._prepare_cache_for_generation(\n",
      "  File \"X:\\Projects\\ZakoniRAG\\.venv\\lib\\site-packages\\transformers\\generation\\utils.py\", line 2092, in _prepare_cache_for_generation\n",
      "    model_kwargs[cache_name] = cache_class(cache_config)\n",
      "  File \"X:\\Projects\\ZakoniRAG\\.venv\\lib\\site-packages\\transformers\\cache_utils.py\", line 971, in __init__\n",
      "    from optimum.quanto import MaxOptimizer, qint2, qint4\n",
      "  File \"X:\\Projects\\ZakoniRAG\\.venv\\lib\\site-packages\\optimum\\quanto\\__init__.py\", line 19, in <module>\n",
      "    from .models import *\n",
      "  File \"X:\\Projects\\ZakoniRAG\\.venv\\lib\\site-packages\\optimum\\quanto\\models\\__init__.py\", line 34, in <module>\n",
      "    from .diffusers_models import *\n",
      "  File \"X:\\Projects\\ZakoniRAG\\.venv\\lib\\site-packages\\optimum\\quanto\\models\\diffusers_models.py\", line 30, in <module>\n",
      "    from diffusers import PixArtTransformer2DModel\n",
      "  File \"<frozen importlib._bootstrap>\", line 1075, in _handle_fromlist\n",
      "  File \"X:\\Projects\\ZakoniRAG\\.venv\\lib\\site-packages\\diffusers\\utils\\import_utils.py\", line 874, in __getattr__\n",
      "    value = getattr(module, name)\n",
      "  File \"X:\\Projects\\ZakoniRAG\\.venv\\lib\\site-packages\\diffusers\\utils\\import_utils.py\", line 873, in __getattr__\n",
      "    module = self._get_module(self._class_to_module[name])\n",
      "  File \"X:\\Projects\\ZakoniRAG\\.venv\\lib\\site-packages\\diffusers\\utils\\import_utils.py\", line 885, in _get_module\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Failed to import diffusers.models.transformers.pixart_transformer_2d because of the following error (look up to see its traceback):\n",
      "DLL load failed while importing _C_flashattention: The specified module could not be found.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting on AI response...\n"
     ]
    }
   ],
   "source": [
    "print(\"Waiting on AI response...\")\n",
    "for chunk in chain.stream(query):\n",
    "    print(chunk, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16547213-6b51-4e4e-8440-004070db7101",
   "metadata": {},
   "source": [
    "### Non-streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "acb06308-ae4f-46d5-9d44-eae92e6f6353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting on AI response...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Waiting on AI response...\")\n",
    "result = chain.invoke(query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c85c23-14cc-4c0b-9593-ea347ede19a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
